# âœ… run_model.py
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import sys
import json
import os
import io

# âœ… Ø¶Ø¨Ø· Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø¹Ù„Ù‰ UTF-8
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

try:
    # âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model_path = "C:/temp/trained_model"
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø±: {model_path}")

    # âœ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ØªÙˆÙƒÙ†ÙŠØ²Ø±
    print("ğŸš€ Ø¬Ø§Ø±ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...", file=sys.stderr)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!", file=sys.stderr)

    # âœ… Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„ÙˆØ³Ø§Ø¦Ø· (arguments) Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† stdin
    if len(sys.argv) < 2:
        raise ValueError("âŒ Ù„Ù… ÙŠØªÙ… ØªÙ„Ù‚ÙŠ Ø£ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª. ØªØ£ÙƒØ¯ Ù…Ù† Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù†Øµ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­.")

    input_data = sys.argv[1]
    print("ğŸ“¥ ØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:", input_data, file=sys.stderr)

    # âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© ØªÙ†Ø³ÙŠÙ‚ JSON
    try:
        data = json.loads(input_data)
        print("âœ… ØªÙ… ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª JSON Ø¨Ù†Ø¬Ø§Ø­!", file=sys.stderr)
    except json.JSONDecodeError as e:
        raise ValueError(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª JSON: {str(e)}")

    # âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù†Øµ ÙˆØµØ­ØªÙ‡
    if not isinstance(data, dict) or "text" not in data:
        raise ValueError("âŒ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ØµØ­ÙŠØ­. ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ JSON Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØªØ§Ø­ 'text'.")

    text = data.get("text", "").strip()
    if not text:
        raise ValueError("âŒ ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ ØµØ§Ù„Ø­ Ù„Ù„ØªØ­Ù„ÙŠÙ„.")

    # âœ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    print("ğŸ¤– Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ...", file=sys.stderr)
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    print("âœ… ØªÙ… ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù†Ù…ÙˆØ°Ø¬!", file=sys.stderr)

    with torch.no_grad():
        outputs = model(**inputs)
        print("âœ… ØªÙ… ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!", file=sys.stderr)
        prediction = torch.argmax(outputs.logits, dim=1).item()

    print("âœ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§ÙƒØªÙ…Ù„ØŒ Ø¬Ø§Ø±ÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ù†ØªÙŠØ¬Ø©...", file=sys.stderr)

    # âœ… Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„Ø­Ø±ÙˆÙ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
    result = {
        "text": text,
        "prediction": prediction
    }
    print(json.dumps(result, ensure_ascii=False))

    print("âœ… ØªÙ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!", file=sys.stderr)

except ValueError as ve:
    error_message = {"error": str(ve)}
    print(json.dumps(error_message, ensure_ascii=False), file=sys.stderr)
    sys.exit(1)

except FileNotFoundError as fe:
    error_message = {"error": str(fe)}
    print(json.dumps(error_message, ensure_ascii=False), file=sys.stderr)
    sys.exit(1)

except Exception as e:
    error_message = {"error": f"âŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {str(e)}"}
    print(json.dumps(error_message, ensure_ascii=False), file=sys.stderr)
    sys.exit(1)
