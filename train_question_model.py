import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import torch
import os

# âœ… 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
data = {
    'text': [
        "Ø§Ø´Ø±Ø­ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ø­Ø±ÙƒÙŠØ©.",
        "Ù…Ø§ Ù‡Ùˆ Ù†Ø§ØªØ¬ Ø¶Ø±Ø¨ 5 Ã— 4ØŸ",
        "Ø§Ø°ÙƒØ± Ø¹Ø§ØµÙ…Ø© ÙØ±Ù†Ø³Ø§.",
        "Ù…Ø§ Ù‡ÙŠ Ù‚ÙˆØ§Ù†ÙŠÙ† Ù†ÙŠÙˆØªÙ† Ù„Ù„Ø­Ø±ÙƒØ©ØŸ",
        "Ø¹Ø±Ù Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø¶ÙˆØ¦ÙŠ."
    ],
    'label': [0, 1, 0, 0, 0]  # 0 = Ø³Ø¤Ø§Ù„ Ù†ØµÙŠØŒ 1 = Ø³Ø¤Ø§Ù„ Ø­Ø³Ø§Ø¨ÙŠ
}

df = pd.DataFrame(data)

# âœ… 2. ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['text'], df['label'], test_size=0.2, random_state=42
)

# âœ… 3. ØªØ­Ù…ÙŠÙ„ Tokenizer ÙˆÙ†Ù…ÙˆØ°Ø¬ BERT
model_name = "bert-base-multilingual-cased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# âœ… 4. ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØµÙˆØµ Ø¥Ù„Ù‰ Ù…Ø¯Ø®Ù„Ø§Øª BERT
train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)
val_encodings = tokenizer(list(val_texts), truncation=True, padding=True)

# âœ… 5. ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„ØªÙ†Ø³ÙŠÙ‚ Dataset
class QuestionDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = QuestionDataset(train_encodings, list(train_labels))
val_dataset = QuestionDataset(val_encodings, list(val_labels))

# âœ… 6. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# âœ… 7. Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    save_strategy="epoch",     # âœ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¹Ø¯ ÙƒÙ„ Epoch
    save_total_limit=1         # âœ… Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø£Ø­Ø¯Ø« Ù†Ù…ÙˆØ°Ø¬ ÙÙ‚Ø·
)

# âœ… 8. Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
print("ğŸš€ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨...")
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

trainer.train()

# âœ… 9. Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ù„Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§
output_dir = "C:/temp/trained_model"  # ğŸ” Ù…Ø³Ø§Ø± Ø¢Ù…Ù† Ù„ØªØ¬Ù†Ø¨ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
    print(f"ğŸ“ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯: {output_dir}")
else:
    print(f"ğŸ“ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ù„ÙØ¹Ù„: {output_dir}")

# âœ… 10. Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„ØªÙˆÙƒÙ†ÙŠØ²Ø± Ù…Ø¹ ØªØµØ­ÙŠØ­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
try:
    print("ğŸ’¾ Ø¬Ø§Ø±ÙŠ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬...")
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)

    # âœ… Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø© Ù„Ù„ØªØ­Ù‚Ù‚
    saved_files = os.listdir(output_dir)
    print("ğŸ“‹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©:", saved_files)

    # âœ… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø£ÙŠ Ù…Ù† Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©
    if "pytorch_model.bin" in saved_files or "model.safetensors" in saved_files:
        print("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­ ÙÙŠ 'C:/temp/trained_model'.")
    else:
        raise FileNotFoundError("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¹Ø¯ Ø§Ù„Ø­ÙØ¸ (pytorch_model.bin Ø£Ùˆ model.safetensors).")
except Exception as e:
    print(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø­Ø§ÙˆÙ„Ø© Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
